{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beebe9cf-d949-440c-8c8a-de91498d55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import traceback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from feeders.feeder_ntu import Feeder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_add_pool\n",
    "import inspect\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Dropout, Linear, Sequential\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.resolver import (\n",
    "    activation_resolver,\n",
    "    normalization_resolver,\n",
    ")\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "from torch_geometric.utils import degree, sort_edge_index\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630af9b4-9dc7-4ad2-9ca8-d1e57dd285c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed):\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.enabled = True\n",
    "    # training speed is too slow if set to True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # on cuda 11 cudnn8, the default algorithm is very slow\n",
    "    # unlike on cuda 10, the default works well\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78031e0e-f212-41a2-8fe9-d2eb91a637b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(phase='train', data='S'):\n",
    "    if phase=='train':\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=Feeder(f'data/ntu/NTU60_C{data}.npz', split='train', p_interval =[0.5, 1], window_size=64),\n",
    "            batch_size=32,\n",
    "            #num_workers=0,\n",
    "            worker_init_fn=init_seed)\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=Feeder(f'data/ntu/NTU60_C{data}.npz', split='test', p_interval =[0.95], window_size=64),\n",
    "            batch_size=32,\n",
    "            #num_workers=0,\n",
    "            worker_init_fn=init_seed)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94eea04d-f170-4b09-8fc2-dee66e57a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGraphCov(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A, **kwargs):\n",
    "        super(SpatialGraphCov, self).__init__()\n",
    "        self.num_subset = A.shape[0]\n",
    "\n",
    "        self.gcn = nn.Conv2d(in_channels, out_channels * A.shape[0], kernel_size=1)\n",
    "        \n",
    "        self.A = torch.tensor(A, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        self.A = self.A.to(x.device)\n",
    "        n, c, t, v = x.size()\n",
    "        # perform gcn\n",
    "        x = self.gcn(x).view(n, self.num_subset, -1, t, v)  # update\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, self.A))  # aggregation\n",
    "        return x\n",
    "\n",
    "class GPSConv(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        conv: Optional[MessagePassing],\n",
    "        heads: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        act: str = 'relu',\n",
    "        att_type: str = 'transformer',\n",
    "        order_by_degree: bool = False,\n",
    "        shuffle_ind: int = 0,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        act_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        norm: Optional[str] = 'batch_norm',\n",
    "        norm_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.conv = conv\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.att_type = att_type\n",
    "        self.shuffle_ind = shuffle_ind\n",
    "        self.order_by_degree = order_by_degree\n",
    "        \n",
    "        assert (self.order_by_degree==True and self.shuffle_ind==0) or (self.order_by_degree==False), f'order_by_degree={self.order_by_degree} and shuffle_ind={self.shuffle_ind}'\n",
    "        \n",
    "        if self.att_type == 'transformer':\n",
    "            self.attn = torch.nn.MultiheadAttention(\n",
    "                channels,\n",
    "                heads,\n",
    "                dropout=attn_dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        if self.att_type == 'mamba':\n",
    "            self.self_attn = Mamba(\n",
    "                d_model=channels,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=1\n",
    "            )\n",
    "            \n",
    "        self.mlp = Sequential(\n",
    "            nn.Conv2d(channels, channels * 2, kernel_size=1),\n",
    "            Dropout(dropout),\n",
    "            nn.Conv2d(channels * 2, channels, kernel_size=1),\n",
    "            Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        norm_kwargs = norm_kwargs or {}\n",
    "        '''self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)'''\n",
    "        self.norm1 = nn.BatchNorm2d(channels)\n",
    "        self.norm2 = nn.BatchNorm2d(channels)\n",
    "        self.norm3 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.norm_with_batch = False\n",
    "        if self.norm1 is not None:\n",
    "            signature = inspect.signature(self.norm1.forward)\n",
    "            self.norm_with_batch = 'batch' in signature.parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        if self.conv is not None:\n",
    "            self.conv.reset_parameters()\n",
    "        self.attn._reset_parameters()\n",
    "        reset(self.mlp)\n",
    "        if self.norm1 is not None:\n",
    "            self.norm1.reset_parameters()\n",
    "        if self.norm2 is not None:\n",
    "            self.norm2.reset_parameters()\n",
    "        if self.norm3 is not None:\n",
    "            self.norm3.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Adj,\n",
    "        batch: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\"\"\"\n",
    "        hs = []\n",
    "        if self.conv is not None:  # Local MPNN.\n",
    "            h = self.conv(x, edge_index, **kwargs)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            h = h + x\n",
    "            if self.norm1 is not None:\n",
    "                if self.norm_with_batch:\n",
    "                    h = self.norm1(h, batch=batch)\n",
    "                else:\n",
    "                    h = self.norm1(h)\n",
    "            hs.append(h)\n",
    "\n",
    "        ### Global attention transformer-style model.\n",
    "        if self.att_type == 'transformer':\n",
    "            h, mask = to_dense_batch(x, batch)\n",
    "            h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)\n",
    "            h = h[mask]\n",
    "            \n",
    "        if self.att_type == 'mamba':\n",
    "            \n",
    "            if self.order_by_degree:\n",
    "                deg = degree(edge_index[0], x.shape[0], dtype=torch.long)\n",
    "                order_tensor = torch.stack([batch, deg], 1).T\n",
    "                _, x = sort_edge_index(order_tensor, edge_attr=x)\n",
    "                \n",
    "            if self.shuffle_ind == 0:\n",
    "                #h, mask = to_dense_batch(x, batch)\n",
    "                N, C, T, V = h.size()\n",
    "                h = h.permute(0, 2, 3, 1).contiguous().view(N*T, V, C)\n",
    "                h = self.self_attn(h)\n",
    "                h = h.contiguous().view(-1, T, V, C).permute(0, 3, 1, 2)\n",
    "            else:\n",
    "                mamba_arr = []\n",
    "                for _ in range(self.shuffle_ind):\n",
    "                    h_ind_perm = permute_within_batch(x, batch)\n",
    "                    h_i, mask = to_dense_batch(x[h_ind_perm], batch)\n",
    "                    h_i = self.self_attn(h_i)[mask][h_ind_perm]\n",
    "                    mamba_arr.append(h_i)\n",
    "                h = sum(mamba_arr) / self.shuffle_ind\n",
    "        ###\n",
    "        \n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = h + x  # Residual connection.\n",
    "        if self.norm2 is not None:\n",
    "            if self.norm_with_batch:\n",
    "                h = self.norm2(h, batch=batch)\n",
    "            else:\n",
    "                h = self.norm2(h)\n",
    "        hs.append(h)\n",
    "\n",
    "        out = sum(hs)  # Combine local and global outputs.\n",
    "\n",
    "        out = out + self.mlp(out)\n",
    "        if self.norm3 is not None:\n",
    "            if self.norm_with_batch:\n",
    "                out = self.norm3(out, batch=batch)\n",
    "            else:\n",
    "                out = self.norm3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.channels}, '\n",
    "                f'conv={self.conv}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf09edbe-6945-42d9-a42f-7c4ca199713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_init(conv):\n",
    "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
    "    nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "\n",
    "def bn_init(bn, scale):\n",
    "    nn.init.constant_(bn.weight, scale)\n",
    "    nn.init.constant_(bn.bias, 0)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        if hasattr(m, 'bias') and m.bias is not None and isinstance(m.bias, torch.Tensor):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        # adjust padding for kernel size so that it will be equal to out_channe;s\n",
    "        pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            # kernel_size, 1 so that we look only for spatial\n",
    "            # 3 time steps windows of only 1 node\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1),\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=5,\n",
    "                 stride=1,\n",
    "                 dilations=[1, 2],\n",
    "                 residual=False,\n",
    "                 residual_kernel_size=1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        # + 2 because we have additional 2 branches for max and 1x1 branch\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "        if type(kernel_size) == list:\n",
    "            assert len(kernel_size) == len(dilations)\n",
    "        else:\n",
    "            kernel_size = [kernel_size] * len(dilations)\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0\n",
    "                ),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=ks,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation\n",
    "                ),\n",
    "            )\n",
    "            # checking for each dilation so that we will look for global context\n",
    "            for ks, dilation in zip(kernel_size, dilations)\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(branch_channels)  # 为什么还要加bn\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride, 1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "        # print(len(self.branches))\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.permute(0,3,1,2)\n",
    "        # Input dim: (N,C,T,V)\n",
    "        res = self.residual(x)\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "        \n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        out += res\n",
    "        #out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "\n",
    "class unit_tcn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super(unit_tcn, self).__init__()\n",
    "        pad = int((kernel_size - 1) / 2)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
    "                              stride=(stride, 1), groups=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        conv_init(self.conv)\n",
    "        bn_init(self.bn, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## n, t, v, c\n",
    "        #x = x.permute(0,3,1,2)\n",
    "        x = self.bn(self.conv(x))\n",
    "        ## n, c, t, v\n",
    "        #x = x.permute(0,2,3,1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2790b42b-faad-477d-a39a-a1dc7122c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEnt(nn.Module):\n",
    "    def __init__(self, dim_in, dim, num_points=25):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_in = dim_in\n",
    "        '''nn1 = Sequential(\n",
    "                nn.Linear(2*dim, dim),\n",
    "            )'''\n",
    "        self_link = [(i, i) for i in range(25)]\n",
    "        inward_ori_index = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21), (6, 5), (7, 6),\n",
    "                    (8, 7), (9, 21), (10, 9), (11, 10), (12, 11), (13, 1),\n",
    "                    (14, 13), (15, 14), (16, 15), (17, 1), (18, 17), (19, 18),\n",
    "                    (20, 19), (22, 8), (23, 8), (24, 12), (25, 12)]\n",
    "        inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "        outward = [(j, i) for (i, j) in inward]\n",
    "        self.neighbor = inward + outward\n",
    "        self.A = self.get_spatial_graph(25, self_link, inward, outward)\n",
    "        self.conv = GPSConv(dim, SpatialGraphCov(dim, dim, self.A), heads=4, attn_dropout=0.5,\n",
    "                       att_type='mamba',\n",
    "                       shuffle_ind=0,\n",
    "                       order_by_degree=True,\n",
    "                       d_state=16, d_conv=4, norm='batch_norm')\n",
    "        #self.edge_emb = nn.Embedding(len(self.neighbor), dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        if dim != dim_in:\n",
    "            self.t = unit_tcn(dim_in, dim, kernel_size=1)\n",
    "        self.joint_label = [0, 4, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 0, 1, 0, 1]\n",
    "        self.pe_proj = nn.Conv2d(dim_in, dim, 1, bias=False)\n",
    "        #self.act = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # N*M, T, V, C\n",
    "        N, C, T, V, M = dims\n",
    "        # HyperEdge\n",
    "        label = F.one_hot(torch.tensor(self.joint_label)).float().to(x.device)\n",
    "        z = x @ (label / label.sum(dim=0, keepdim=True))\n",
    "        z = self.pe_proj(z).permute(3, 0, 1, 2)\n",
    "        e = z[self.joint_label].permute(1, 2, 3, 0).contiguous()\n",
    "        #increase in demension\n",
    "        if self.dim_in != self.dim:\n",
    "            x = self.t(x)\n",
    "        '''N * M - number of video sequences with person number\n",
    "        C - number of channels (3d position of points)\n",
    "        T - number of frames\n",
    "        V - number of skeleton points (25)\n",
    "        order is: N*M, T, V, C\n",
    "        '''\n",
    "        NM, C, T, V = x.size() \n",
    "        batch = self.create_batch_array(N*M, 1, 1, x.device)\n",
    "        edge_index = self.convert_neighbor_to_edge_index(self.neighbor, x.device)\n",
    "        #edge_index = self.repeat_tensor_with_increment(edge_index, N*M, V)\n",
    "        #edge_attr = torch.ones(edge_index.size(1), dtype=torch.int, device=x.device)\n",
    "        #edge_attr = self.edge_emb(edge_attr)\n",
    "        # Sum\n",
    "        ot = x+e\n",
    "        ot = self.conv(ot, edge_index, batch).contiguous()\n",
    "        return ot\n",
    "\n",
    "\n",
    "    def convert_neighbor_to_edge_index(self, neighbor, device):\n",
    "        indices = torch.tensor(neighbor, dtype=torch.int64, device=device).t()\n",
    "        return indices\n",
    "\n",
    "    def repeat_tensor_with_increment(self, tensor, batch_size, V):\n",
    "        result = tensor\n",
    "        for i in range(1, batch_size):\n",
    "            new_tensor = tensor + V*i\n",
    "            result = torch.cat((result, new_tensor), dim=1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def create_batch_array(self, NM, T, V, device):\n",
    "        # Total number of unique indices\n",
    "        num_indices = NM\n",
    "        # Create a tensor of shape (num_indices, V) where each row contains the same index\n",
    "        batch = torch.arange(num_indices, dtype=torch.int64, device=device).repeat_interleave(V * T)\n",
    "        return batch\n",
    "\n",
    "    def edge2mat(self, link, num_node):\n",
    "        A = np.zeros((num_node, num_node))\n",
    "        for i, j in link:\n",
    "            A[j, i] = 1\n",
    "        return A\n",
    "    \n",
    "    def normalize_digraph(self, A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        h, w = A.shape\n",
    "        Dn = np.zeros((w, w))\n",
    "        for i in range(w):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i] ** (-1)\n",
    "        AD = np.dot(A, Dn)\n",
    "        return AD\n",
    "    \n",
    "    def get_spatial_graph(self, num_node, self_link, inward, outward):\n",
    "        I = self.edge2mat(self_link, num_node)\n",
    "        In = self.normalize_digraph(self.edge2mat(inward, num_node))\n",
    "        Out = self.normalize_digraph(self.edge2mat(outward, num_node))\n",
    "        A = np.stack((I, In, Out))\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86eeaea-fc3b-48ea-97b6-fdf8a3b5e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim, stride=1):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim = dim\n",
    "        self.conv = GraphEnt(dim_in, dim)\n",
    "        self.tcn = MultiScale_TemporalConv(dim, dim, kernel_size=5, stride=stride,\n",
    "                                            dilations=[1,2],\n",
    "                                            # residual=True has worse performance in the end\n",
    "                                            residual=False)\n",
    "        self.act = nn.Tanh()\n",
    "        self.norm = nn.BatchNorm2d(dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        if dim_in == dim and stride == 1:\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = unit_tcn(dim_in, dim, kernel_size=1, stride=stride)\n",
    "    \n",
    "\n",
    "    def forward(self, x, dims):\n",
    "        x = self.tcn(self.conv(x, dims)) + self.residual(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.act(self.norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653e9fba-b372-4a16-91b2-372abf348391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, dim_in, dim):\n",
    "        super().__init__()\n",
    "        self.l1 = GraphTCN(dim_in, dim)\n",
    "        self.l2 = GraphTCN(dim, dim)\n",
    "        self.l3 = GraphTCN(dim, dim)\n",
    "        self.l4 = GraphTCN(dim, dim)\n",
    "        self.l5 = GraphTCN(dim, dim)\n",
    "        '''self.l1 = GraphTCN(dim_in, dim)\n",
    "        self.l2 = GraphTCN(dim, dim)\n",
    "        self.l3 = GraphTCN(dim, dim*2, stride=2)\n",
    "        self.l4 = GraphTCN(dim*2,dim*2)\n",
    "        self.l5 = GraphTCN(dim*2, dim*2)'''\n",
    "        self.fc1 = nn.Linear(dim, 60)\n",
    "        self.mlp = nn.Sequential(\n",
    "            self.fc1\n",
    "        )\n",
    "        self.data_bn = nn.BatchNorm1d(2*3*25)\n",
    "        nn.init.normal_(self.fc1.weight, 0, math.sqrt(2. / 60))\n",
    "        bn_init(self.data_bn, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, T, V, M = x.size()\n",
    "        dims = x.size()\n",
    "        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(N, M, V, C, T).contiguous().view(N * M, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
    "        # N*M, C, T, V\n",
    "        x = self.l1(x, dims)\n",
    "        x = self.l2(x, dims)\n",
    "        x = self.l3(x, dims)\n",
    "        x = self.l4(x, dims)\n",
    "        x = self.l5(x, dims)\n",
    "        '''x = self.l6(x, dims)\n",
    "        x = self.l7(x, dims)\n",
    "        x = self.l8(x, dims)\n",
    "        x = self.l9(x, dims)\n",
    "        x = self.l10(x, dims)'''\n",
    "        '''\n",
    "        order is: N*M, T, V, C\n",
    "        '''\n",
    "        #x = x.permute(0,3,1,2)\n",
    "        _, C, T, V = x.size()\n",
    "        x = x.view(N, M, C, -1)\n",
    "        # order is: N, M, C, T*V\n",
    "        x = x.mean(3).mean(1)\n",
    "        x = self.mlp(x)\n",
    "        #print(\"RESULT\", x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5af8e0-4184-45c4-b731-62f2b96289ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_value = []\n",
    "    acc_value = []\n",
    "    train_loader = load_data('train')\n",
    "    process = tqdm(train_loader, ncols=80)\n",
    "    for batch_idx, (data, label, index) in enumerate(process):\n",
    "        with torch.no_grad():\n",
    "            data = data.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "        out = model(data)\n",
    "        loss = lossC(out, target=label)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # Step the optimizer\n",
    "        optimizer.step()\n",
    "        loss_value.append(loss.data.item())\n",
    "        value, predict_label = torch.max(out.data, 1)\n",
    "        acc = torch.mean((predict_label == label.data).float())\n",
    "        acc_value.append(acc.data.item())\n",
    "    return np.nanmean(loss_value), np.nanmean(acc_value)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d66187d-dc25-421a-b1a7-d4d553fe003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalt(wrong_file = None, result_file = None, model_name = 'Model', save=True, loader=load_data('test')):\n",
    "    if wrong_file is not None:\n",
    "        f_w = open(wrong_file, 'w')\n",
    "    if result_file is not None:\n",
    "        f_r = open(result_file, 'w')\n",
    "    model.eval()\n",
    "    test_loader = loader\n",
    "    loss_value = []\n",
    "    score_frag = []\n",
    "    process = tqdm(test_loader, ncols=80)\n",
    "    for batch_idx, (data, label, index) in enumerate(process):\n",
    "        with torch.no_grad():\n",
    "            data = data.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "            out = model(data)\n",
    "            loss = lossC(out, target=label)\n",
    "            #print(out.data.cpu().numpy())\n",
    "            value, predict_label = torch.max(out.data, 1)\n",
    "            score_frag.append(out.data.cpu().numpy())\n",
    "            loss_value.append(loss.data.item())\n",
    "            _, predict_label = torch.max(out.data, 1)\n",
    "        if wrong_file is not None or result_file is not None:\n",
    "            predict = list(predict_label.cpu().numpy())\n",
    "            true = list(label.data.cpu().numpy())\n",
    "            for i, x in enumerate(predict):\n",
    "                if result_file is not None:\n",
    "                    f_r.write(str(x) + ',' + str(true[i]) + '\\n')\n",
    "                if x != true[i] and wrong_file is not None:\n",
    "                    f_w.write(str(index[i]) + ',' + str(x) + ',' + str(true[i]) + '\\n')\n",
    "    score = np.concatenate(score_frag)\n",
    "    loss = np.nanmean(loss_value)\n",
    "    top5_acc = test_loader.dataset.top_k(score, 5) * 100\n",
    "    top1_acc = test_loader.dataset.top_k(score, 1) * 100\n",
    "    global best_epoch_acc\n",
    "    if save and top1_acc > best_epoch_acc:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, f'mainruns/{model_name}.pt')\n",
    "        best_epoch_acc = top1_acc\n",
    "    \n",
    "    print('\\tTop{}: {:.2f}%'.format(\n",
    "            1, top1_acc))\n",
    "    print('\\tTop{}: {:.2f}%'.format(\n",
    "            5, top5_acc))\n",
    "    return top5_acc, loss, top1_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bc7662e-6ac5-41a2-84ba-7c69c6d918f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def adjust_learning_rate(epoch):\n",
    "    global lr, warm_epochs, lr_decay_rate, step\n",
    "    if epoch <= warm_epochs:\n",
    "        lr = base_lr * epoch / warm_epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        if sched == 'plat':\n",
    "            scheduler.step(tst[1])\n",
    "        elif sched == 'custom':\n",
    "            lr = base_lr * (lr_decay_rate ** np.sum(epoch >= np.array(step)))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3d47e-d029-44d0-abdf-296261f90423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 2400936\n",
      "GraphModel(\n",
      "  (l1): GraphTCN(\n",
      "    (conv): GraphEnt(\n",
      "      (conv): GPSConv(192, conv=SpatialGraphCov(\n",
      "        (gcn): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      ), heads=4)\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (t): unit_tcn(\n",
      "        (conv): Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (pe_proj): Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): Tanh()\n",
      "    (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (residual): unit_tcn(\n",
      "      (conv): Conv2d(3, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (l2): GraphTCN(\n",
      "    (conv): GraphEnt(\n",
      "      (conv): GPSConv(192, conv=SpatialGraphCov(\n",
      "        (gcn): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      ), heads=4)\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (pe_proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): Tanh()\n",
      "    (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (l3): GraphTCN(\n",
      "    (conv): GraphEnt(\n",
      "      (conv): GPSConv(192, conv=SpatialGraphCov(\n",
      "        (gcn): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      ), heads=4)\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (pe_proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): Tanh()\n",
      "    (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (l4): GraphTCN(\n",
      "    (conv): GraphEnt(\n",
      "      (conv): GPSConv(192, conv=SpatialGraphCov(\n",
      "        (gcn): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      ), heads=4)\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (pe_proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): Tanh()\n",
      "    (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (l5): GraphTCN(\n",
      "    (conv): GraphEnt(\n",
      "      (conv): GPSConv(192, conv=SpatialGraphCov(\n",
      "        (gcn): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "      ), heads=4)\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (pe_proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (tcn): MultiScale_TemporalConv(\n",
      "      (branches): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): TemporalConv(\n",
      "            (conv): Conv2d(48, 48, kernel_size=(5, 1), stride=(1, 1), padding=(4, 0), dilation=(2, 1))\n",
      "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), dilation=1, ceil_mode=False)\n",
      "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (act): Tanh()\n",
      "    (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=192, out_features=60, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=60, bias=True)\n",
      "  )\n",
      "  (data_bn): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████▉                       | 530/1253 [02:41<03:38,  3.31it/s]"
     ]
    }
   ],
   "source": [
    "init_seed(2)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "#import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "phase = 'train'\n",
    "optimz = 'SGD'\n",
    "warm_epochs = 5\n",
    "epochs = 140\n",
    "out_channels = 192\n",
    "weight_decay = 1e-4\n",
    "if optimz == 'Adam' or optimz == 'AdamW' or optimz == 'NAdam':\n",
    "    base_lr = 0.0025\n",
    "    lr = base_lr\n",
    "    lr_decay_rate = 0.2\n",
    "    step = [10, 25, 40, 60]\n",
    "    sched = 'plat'\n",
    "elif optimz == 'SGD':\n",
    "    base_lr = 0.01\n",
    "    lr = base_lr\n",
    "    lr_decay_rate = 0.2\n",
    "    step = [60, 100]\n",
    "    sched = 'custom'\n",
    "\n",
    "use_wandb = False\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphModel(3, out_channels).to(device)\n",
    "# model.apply(init_weights)\n",
    "'''devices = [2, 3]\n",
    "device = devices[0]\n",
    "model = GraphModel(3, out_channels)\n",
    "model = nn.DataParallel(model,\n",
    "                        device_ids=devices,\n",
    "                        output_device=device)\n",
    "model.to(device)\n",
    "model = nn.SyncBatchNorm.convert_sync_batchnorm(model)'''\n",
    "\n",
    "if optimz == 'SGD':\n",
    "    optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                momentum=0.9,\n",
    "                lr=base_lr,\n",
    "                nesterov = True,\n",
    "                weight_decay = weight_decay)\n",
    "elif optimz == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=base_lr)\n",
    "elif optimz == 'NAdam':\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "elif optimz == 'AdamW':\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "'''for name, parameter in model.named_parameters():\n",
    "    if not parameter.requires_grad:\n",
    "        continue\n",
    "    params = parameter.numel()\n",
    "    print(name, params)'''\n",
    "lossC = nn.CrossEntropyLoss().to(device)\n",
    "# scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "if sched == 'plat':\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_rate, patience=5)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma=0.7)\n",
    "if phase == 'train':\n",
    "    if use_wandb:\n",
    "        wandb.login()\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"spanchsan-hong-kong-polytechnic-university\",\n",
    "        \n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"Mamba\",\n",
    "            \"dataset\": \"NTU RGB+D 60\",\n",
    "            \"epochs\": epochs,\n",
    "            \"out_channels\": out_channels,\n",
    "            \"batch_size\": 64,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr_decay_rate\": lr_decay_rate,\n",
    "            \"optimizer\": optimz,\n",
    "            \"scheduler\": sched\n",
    "            }\n",
    "        )\n",
    "    print(\"Parameters:\", count_parameters(model))\n",
    "    print(model)\n",
    "    best_epoch_acc = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        adjust_learning_rate(epoch)\n",
    "        trn = train(epoch)\n",
    "        if use_wandb:\n",
    "            tst = evalt(model_name = f'Model{out_channels}_{wandb.run.name}')\n",
    "        else:\n",
    "            tst = evalt(save = False)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train_loss\": trn[0], \"train_acc\": trn[1], \"test_acc\": tst[0], \"test_acc_1st\": tst[2], \"test_loss\": tst[1]})\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {trn[0]:.4f}, Train Acc: {trn[1]:.4f}, \\\n",
    "        Accuracy: {tst[2]:.4f}, Test_Loss: {tst[1]:.4f}, Current_Lr: {current_lr:.6f}')\n",
    "elif phase == 'test':\n",
    "    print(\"Parameters:\", count_parameters(model))\n",
    "    weights_path = 'mainruns/Model192_winter-water-141.pt'\n",
    "    model.load_state_dict(torch.load(weights_path, weights_only=True))\n",
    "    wf = weights_path.replace('.pt', '_wrong.txt')\n",
    "    rf = weights_path.replace('.pt', '_right.txt')\n",
    "    loader = load_data(phase='test', data='S')\n",
    "    evalt(wrong_file=wf, result_file=rf, save=False, loader=loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798225ed-8ee2-44ec-a724-382d278481a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #self.joint_label = [0, 4, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 0, 1, 0, 1]\n",
    "        #self.pe_proj = nn.Conv2d(dim_in, dim, 1, bias=False)\n",
    "'''\n",
    "        # HyperEdge\n",
    "        x0 = x.permute(0, 3, 1, 2) \n",
    "        label = F.one_hot(torch.tensor(self.joint_label)).float().to(x.device)\n",
    "        z = x0 @ (label / label.sum(dim=0, keepdim=True))\n",
    "        z = self.pe_proj(z).permute(3, 0, 1, 2)\n",
    "        e = z[self.joint_label].permute(1, 3, 0, 2).contiguous()\n",
    "        # Sum\n",
    "        ot = x+e\n",
    "        ot = self.norm(ot)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff6836-6547-47a5-a7bd-6ee872115bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
