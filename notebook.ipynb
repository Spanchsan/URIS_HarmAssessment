{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebe9cf-d949-440c-8c8a-de91498d55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import traceback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from feeders.feeder_ntu import Feeder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR, LinearLR\n",
    "from ptflops import get_model_complexity_info\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Dropout, Linear, Sequential\n",
    "\n",
    "from mamba_ssm import Mamba2\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630af9b4-9dc7-4ad2-9ca8-d1e57dd285c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed):\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.enabled = True\n",
    "    # training speed is too slow if set to True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # on cuda 11 cudnn8, the default algorithm is very slow\n",
    "    # unlike on cuda 10, the default works well\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09edbe-6945-42d9-a42f-7c4ca199713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_init(conv):\n",
    "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
    "    nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "\n",
    "def bn_init(bn, scale):\n",
    "    nn.init.constant_(bn.weight, scale)\n",
    "    nn.init.constant_(bn.bias, 0)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        if hasattr(m, 'bias') and m.bias is not None and isinstance(m.bias, torch.Tensor):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        # adjust padding for kernel size so that it will be equal to out_channe;s\n",
    "        pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            # kernel_size, 1 so that we look only for spatial\n",
    "            # 3 time steps windows of only 1 node\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1),\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=5,\n",
    "                 stride=1,\n",
    "                 dilations=[1, 2],\n",
    "                 residual=False,\n",
    "                 residual_kernel_size=1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        # + 2 because we have additional 2 branches for max and 1x1 branch\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "        if type(kernel_size) == list:\n",
    "            assert len(kernel_size) == len(dilations)\n",
    "        else:\n",
    "            kernel_size = [kernel_size] * len(dilations)\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0\n",
    "                ),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=ks,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation\n",
    "                ),\n",
    "            )\n",
    "            # checking for each dilation so that we will look for global context\n",
    "            for ks, dilation in zip(kernel_size, dilations)\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(branch_channels)  # 为什么还要加bn\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride, 1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "        # print(len(self.branches))\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        res = self.residual(x)\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "        \n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        out += res\n",
    "        return out\n",
    "\n",
    "class unit_tcn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super(unit_tcn, self).__init__()\n",
    "        pad = int((kernel_size - 1) / 2)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
    "                              stride=(stride, 1), groups=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        conv_init(self.conv)\n",
    "        bn_init(self.bn, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.conv(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679daded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdc(A: sp.csr_matrix, alpha: float, eps: float):\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Self-loops\n",
    "    A_loop = sp.eye(N) + A\n",
    "\n",
    "    # Symmetric transition matrix\n",
    "    D_loop_vec = A_loop.sum(0).A1\n",
    "    D_loop_vec_invsqrt = 1 / np.sqrt(D_loop_vec)\n",
    "    D_loop_invsqrt = sp.diags(D_loop_vec_invsqrt)\n",
    "    T_sym = D_loop_invsqrt @ A_loop @ D_loop_invsqrt\n",
    "\n",
    "    # PPR-based diffusion\n",
    "    S = alpha * sp.linalg.inv(sp.eye(N) - (1 - alpha) * T_sym)\n",
    "\n",
    "    # Sparsify using threshold epsilon\n",
    "    S_tilde = S.multiply(S >= eps)\n",
    "\n",
    "    # Column-normalized transition matrix on graph S_tilde\n",
    "    D_tilde_vec = S_tilde.sum(0).A1\n",
    "    T_S = S_tilde / D_tilde_vec\n",
    "    \n",
    "    return T_S\n",
    "\n",
    "def edge2mat(link, num_node):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in link:\n",
    "        A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "def normalize_digraph( A):\n",
    "    Dl = np.sum(A, 0)\n",
    "    h, w = A.shape\n",
    "    Dn = np.zeros((w, w))\n",
    "    for i in range(w):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "def get_spatial_graph( num_node, self_link, inward, outward):\n",
    "    I = edge2mat(self_link, num_node)\n",
    "    In = normalize_digraph(edge2mat(inward, num_node))\n",
    "    Out = normalize_digraph(edge2mat(outward, num_node))\n",
    "    A = np.stack((I, In, Out))\n",
    "    return A\n",
    "\n",
    "def construct_incidence_matrix(A):\n",
    "    v = A.shape[0]\n",
    "    edge_list = []\n",
    "    for i in range(v):\n",
    "        for j in range(i + 1, v):\n",
    "            if A[i, j] != 0:\n",
    "                edge_list.append((i, j))\n",
    "    e = len(edge_list)\n",
    "    I = torch.zeros((v, e))\n",
    "    B = torch.zeros((e, e))\n",
    "    for e, (i, j) in enumerate(edge_list):\n",
    "        I[i, e] = 1\n",
    "        I[j, e] = 1\n",
    "    for i in range(e):\n",
    "        for j in range(e):\n",
    "            if edge_list[i][0] in edge_list[j] or edge_list[i][1] in edge_list[j]:\n",
    "                B[i, j] = 1\n",
    "                B[j, i] = 1\n",
    "    return I, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eea04d-f170-4b09-8fc2-dee66e57a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParalleGraphCov(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A, **kwargs):\n",
    "        super(ParalleGraphCov, self).__init__()\n",
    "        '''self.num_subset = A.shape[0]\n",
    "\n",
    "        self.gcn = nn.Conv2d(in_channels, out_channels * A.shape[0], kernel_size=1)'''\n",
    "        self.A = torch.tensor(A, dtype=torch.float32, requires_grad=False)\n",
    "        self.num_heads = 8\n",
    "        self.fc2 = nn.ModuleList([nn.Conv2d(in_channels, out_channels, 1, groups=self.num_heads) for _ in range(3)])\n",
    "        self.fc1 = nn.Parameter(torch.stack([torch.stack([torch.eye(A.shape[-1]) for _ in range(self.num_heads)], dim=0) for _ in range(3)], dim=0), requires_grad=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.A = self.A.to(x.device)\n",
    "        N, C, T, V = x.size()\n",
    "        # perform gcn\n",
    "        #x = self.gcn(x).view(N, self.num_subset, -1, T, V)  # update\n",
    "        #x = torch.einsum('nkctv,kvw->nctw', (x, self.A))  # aggregation\n",
    "        y = None\n",
    "        for i in range(3):\n",
    "            w1 = self.fc1[i]\n",
    "            x = x.view(N, self.num_heads, -1, T, V)\n",
    "            z = torch.einsum(\"nhctv, hvw->nhctw\", (x, w1)).contiguous().view(N, -1, T, V)\n",
    "\n",
    "            z = self.fc2[i](z)\n",
    "\n",
    "            y = z + y if y is not None else z\n",
    "        return y\n",
    "\n",
    "class STGC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A, **kwargs):\n",
    "        super(STGC, self).__init__()\n",
    "        self.num_subset = A.shape[0]\n",
    "\n",
    "        self.gcn = nn.Conv2d(in_channels, out_channels * A.shape[0], kernel_size=1)\n",
    "        self.A = torch.tensor(A, dtype=torch.float32, requires_grad=False).clone().detach()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.A = self.A.to(x.device)\n",
    "        N, C, T, V = x.size()\n",
    "        # perform gcn\n",
    "        x = self.gcn(x).view(N, self.num_subset, -1, T, V)  # update\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, self.A))  # aggregation\n",
    "        return x\n",
    "        \n",
    "\n",
    "class GCNMamba(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim,\n",
    "        A,\n",
    "        line_repr,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.line_repr = line_repr\n",
    "        '''self.self_attn = Mamba2(\n",
    "            d_model=dim,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            headdim=dim//8,\n",
    "            expand=1,\n",
    "        )\n",
    "        self.hops, self.rpe = construct_hop_rpe(h1)\n",
    "        self.joint_label = [0, 4, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 0, 1, 0, 1]\n",
    "        self.pe_proj = nn.Conv2d(dim_in, dim, 1, bias=False)\n",
    "        '''\n",
    "        self.norm1 = nn.BatchNorm2d(dim)\n",
    "        if line_repr:\n",
    "            self.conv = STGC(dim_in, dim, np.array([A]))\n",
    "            self.learnableA = nn.Parameter(torch.zeros((A.shape[1], dim)))\n",
    "            self.A = torch.tensor(A).clone().detach().long()\n",
    "        else:\n",
    "            self.conv = STGC(dim_in, dim, A)\n",
    "\n",
    "    def forward(self, x, dims) -> Tensor:\n",
    "        #Mamba\n",
    "        _, C, T, V = x.size()\n",
    "        N, _, _, _, M = dims\n",
    "        if self.line_repr:\n",
    "            learn_repr = self.learnableA[self.A].permute(2, 0, 1)\n",
    "            x = torch.einsum(\"bctv, cvw -> bctw\", x, learn_repr)\n",
    "        x = x.reshape(N, C, T*M, V)\n",
    "        x = self.conv(x)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(N*M, C, T, V)\n",
    "        return x\n",
    "\n",
    "    def construct_rpe_hops(self, h1):\n",
    "        h = [None for _ in range(25)]\n",
    "        h[0] = np.eye(25)\n",
    "        h[1] = h1\n",
    "        hops = 0*h[0]\n",
    "        for i in range(2, 25):\n",
    "            h[i] = h[i-1] @ h1.transpose(0, 1)\n",
    "            h[i][h[i] != 0] = 1\n",
    "        \n",
    "        for i in range(25-1, 0, -1):\n",
    "            if np.any(h[i]-h[i-1]):\n",
    "                h[i] = h[i] - h[i - 1]\n",
    "                hops += i*h[i]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        hops = torch.tensor(hops).long()\n",
    "        rpe = nn.Parameter(torch.zeros((self.hops.max()+1, dim)))\n",
    "        return hops, rpe\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.dim}, '\n",
    "                f'd_state={self.d_state}, d_conv={self.d_conv})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790b42b-faad-477d-a39a-a1dc7122c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEnt(nn.Module):\n",
    "    def __init__(self, dim_in, dim, A, line_repr):\n",
    "        super().__init__()\n",
    "        #SpatialGraphCov(dim, dim, self.A)\n",
    "        self.conv = GCNMamba(dim, dim, A, line_repr, d_state=64, d_conv=4) # changed number of param\n",
    "        self.act = nn.ReLU(inplace = True)\n",
    "        #self.norm_e = nn.BatchNorm2d(dim)\n",
    "        self.dim_up = unit_tcn(dim_in, dim, kernel_size=1)\n",
    "\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # N*M, T, V, C\n",
    "        N, C, T, V, M = dims\n",
    "        #e = self.norm_e(e)\n",
    "        '''N * M - number of video sequences with person number\n",
    "        C - number of channels (3d position of points)\n",
    "        T - number of frames\n",
    "        V - number of skeleton points (25)\n",
    "        order is: N*M, T, V, C\n",
    "        '''\n",
    "        x = self.dim_up(x) \n",
    "        # Sum\n",
    "        x = self.conv(x, dims).contiguous()\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86eeaea-fc3b-48ea-97b6-fdf8a3b5e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim, A, stride=1, line_repr = False):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim = dim\n",
    "        self.line_repr = line_repr\n",
    "        self.conv = GraphEnt(dim_in, dim, A, line_repr)\n",
    "        self.tcn = MultiScale_TemporalConv(dim, dim, kernel_size=5, stride=stride,\n",
    "                                            dilations=[1,2],\n",
    "                                            # residual=True has worse performance in the end\n",
    "                                            residual=False)\n",
    "        self.act = nn.ReLU(inplace = True)\n",
    "        if dim_in == dim and stride == 1:\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = unit_tcn(dim_in, dim, kernel_size=1, stride=stride)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x, dims):  \n",
    "        x = self.tcn(self.conv(x, dims)) + self.residual(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e9fba-b372-4a16-91b2-372abf348391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, dim_in, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # line_repr and A\n",
    "        self_link = [(i, i) for i in range(25)]\n",
    "        inward_ori_index = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21), (6, 5), (7, 6),\n",
    "                    (8, 7), (9, 21), (10, 9), (11, 10), (12, 11), (13, 1),\n",
    "                    (14, 13), (15, 14), (16, 15), (17, 1), (18, 17), (19, 18),\n",
    "                    (20, 19), (22, 23), (23, 8), (24, 25), (25, 12)]\n",
    "        inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "        outward = [(j, i) for (i, j) in inward]\n",
    "        self.neighbor = inward + outward\n",
    "        self.A = get_spatial_graph(25, self_link, inward, outward)\n",
    "        h1 = self.A.sum(0)\n",
    "        h1[h1 != 0] = 1\n",
    "        #h1 = gdc(sp.csr_matrix(h1), 0.15, 5e-2).toarray()\n",
    "        self.I_edge, B = construct_incidence_matrix(h1)\n",
    "\n",
    "        self.l1 = GraphTCN(dim_in, dim, self.A)\n",
    "        self.l2 = GraphTCN(dim, dim, self.A)\n",
    "        self.l3 = GraphTCN(dim, dim, self.A)\n",
    "        self.l4 = GraphTCN(dim, dim, self.A)\n",
    "        self.l5 = GraphTCN(dim, dim*2, self.A, stride=2)\n",
    "        self.l6 = GraphTCN(dim*2, dim*2, self.A)\n",
    "        self.l7 = GraphTCN(dim*2, dim*2, self.A)\n",
    "        self.l8 = GraphTCN(dim*2, dim*4, self.A, stride=2)\n",
    "        self.l9 = GraphTCN(dim*4, dim*4, self.A)\n",
    "        self.l10 = GraphTCN(dim*4, dim*4, self.A)\n",
    "\n",
    "        # for line repr\n",
    "        self.r1 = GraphTCN(dim_in, dim, B, line_repr=True)\n",
    "        self.r2 = GraphTCN(dim, dim, B, line_repr=True)\n",
    "        self.r3 = GraphTCN(dim, dim, B, line_repr=True)\n",
    "        self.r4 = GraphTCN(dim, dim, B, line_repr=True)\n",
    "        self.r5 = GraphTCN(dim, dim, B, line_repr=True)\n",
    "        self.r6 = GraphTCN(dim, dim*2, B, stride = 2, line_repr=True)\n",
    "        self.r7 = GraphTCN(dim*2, dim*2, B, line_repr=True)\n",
    "        self.r8 = GraphTCN(dim*2, dim*2, B, line_repr=True)\n",
    "        self.r9 = GraphTCN(dim*2, dim*4, B, stride=2, line_repr=True)\n",
    "        self.r10 = GraphTCN(dim*4, dim*4, B, line_repr=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(dim*4, 60)\n",
    "        self.mlp = nn.Sequential(\n",
    "            self.fc1\n",
    "        )\n",
    "        self.data_bn = nn.BatchNorm1d(2*3*25)\n",
    "        nn.init.normal_(self.fc1.weight, 0, math.sqrt(2. / 60))\n",
    "        bn_init(self.data_bn, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, T, V, M = x.size()\n",
    "        dims = x.size()\n",
    "        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n",
    "        x = self.data_bn(x)\n",
    "        x = x.view(N, M, V, C, T).contiguous().view(N * M, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        # line graph representation\n",
    "        self.I_edge = self.I_edge.to(x.device)\n",
    "        y = torch.einsum(\"bctv, ve -> bcte\", x, self.I_edge)\n",
    "        # N*M, C, T, V\n",
    "        y = self.r1(y, dims)\n",
    "        x = self.l1(x, dims) + self.convert_line_to_x(y, self.I_edge)\n",
    "\n",
    "        y = self.r2(y, dims)\n",
    "        x = self.l2(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r3(y, dims)\n",
    "        x = self.l3(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r4(y, dims)\n",
    "        x = self.l4(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r5(y, dims)\n",
    "        x = self.l5(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r6(y, dims)\n",
    "        x = self.l6(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r7(y, dims)\n",
    "        x = self.l7(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r8(y, dims)\n",
    "        x = self.l8(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r9(y, dims)\n",
    "        x = self.l9(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "\n",
    "        y = self.r10(y, dims)\n",
    "        x = self.l10(x + self.convert_line_to_x(y, self.I_edge), dims)\n",
    "        \n",
    "        '''x = self.l6(x, dims)\n",
    "        x = self.l7(x, dims)\n",
    "        x = self.l8(x, dims)\n",
    "        x = self.l9(x, dims)\n",
    "        x = self.l10(x, dims)'''\n",
    "        '''\n",
    "        order is: N*M, T, V, C\n",
    "        '''\n",
    "        #x = x.permute(0,3,1,2)\n",
    "        _, C, T, V = x.size()\n",
    "        x = x.view(N, M, C, -1)\n",
    "        # order is: N, M, C, T*V\n",
    "        x = x.mean(3).mean(1)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def convert_line_to_x(self, y, I):\n",
    "        x = torch.einsum(\"bcte, ve -> bctv\", y, I)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5af8e0-4184-45c4-b731-62f2b96289ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loader):\n",
    "    model.train()\n",
    "    loss_value = []\n",
    "    acc_value = []\n",
    "    process = tqdm(loader, ncols=80)\n",
    "    for batch_idx, (data, label, index) in enumerate(process):\n",
    "        with torch.no_grad():\n",
    "            data = data.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "        out = model(data)\n",
    "        loss = lossC(out, target=label)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # Step the optimizer\n",
    "        optimizer.step()\n",
    "        loss_value.append(loss.data.item())\n",
    "        value, predict_label = torch.max(out.data, 1)\n",
    "        acc = torch.mean((predict_label == label.data).float())\n",
    "        acc_value.append(acc.data.item())\n",
    "    return np.nanmean(loss_value), np.nanmean(acc_value)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66187d-dc25-421a-b1a7-d4d553fe003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalt(loader, wrong_file = None, result_file = None, model_name = 'Model', save=True, save_pickle=False):\n",
    "    if wrong_file is not None:\n",
    "            f_w = open(wrong_file, 'w')\n",
    "    if result_file is not None:\n",
    "        f_r = open(result_file, 'w')\n",
    "    model.eval()\n",
    "    #print('Eval epoch: {}'.format(epoch + 1))\n",
    "    loss_value = []\n",
    "    score_frag = []\n",
    "    show_topk = [1, 5]\n",
    "    process = tqdm(loader, ncols=80)\n",
    "    for batch_idx, (data, label, index) in enumerate(process):\n",
    "        with torch.no_grad():\n",
    "            data = data.float().cuda(device)\n",
    "            label = label.long().cuda(device)\n",
    "            output = model(data)\n",
    "\n",
    "            loss = lossC(output, label)\n",
    "\n",
    "            score_frag.append(output.data.cpu().numpy())\n",
    "            loss_value.append(loss.data.item())\n",
    "\n",
    "            _, predict_label = torch.max(output.data, 1)\n",
    "\n",
    "        if wrong_file is not None or result_file is not None:\n",
    "            predict = list(predict_label.cpu().numpy())\n",
    "            true = list(label.data.cpu().numpy())\n",
    "            for i, x in enumerate(predict):\n",
    "                if result_file is not None:\n",
    "                    f_r.write(str(x) + ',' + str(true[i]) + '\\n')\n",
    "                if x != true[i] and wrong_file is not None:\n",
    "                    f_w.write(str(index[i]) + ',' + str(x) + ',' + str(true[i]) + '\\n')\n",
    "\n",
    "    score = np.concatenate(score_frag)\n",
    "    loss = np.mean(loss_value)\n",
    "    accuracy = loader.dataset.top_k(score, 1)\n",
    "    print('Accuracy: ', accuracy, ' model: ', model_name)\n",
    "    print('\\tMean {} loss of {} batches: {}.'.format(\n",
    "        \"test\", len(loader), np.mean(loss_value)))\n",
    "    for k in show_topk:\n",
    "        print('\\tTop{}: {:.2f}%'.format(\n",
    "            k, 100 * loader.dataset.top_k(score, k)))\n",
    "\n",
    "    top5_acc = loader.dataset.top_k(score, 5) * 100\n",
    "    top1_acc = loader.dataset.top_k(score, 1) * 100\n",
    "    global best_epoch_acc\n",
    "    if save and top1_acc > best_epoch_acc:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, f'mainruns/{model_name}.pt')\n",
    "        best_epoch_acc = top1_acc\n",
    "    if save_pickle:\n",
    "        score_dict = dict(\n",
    "                    zip(loader.dataset.sample_name, score))\n",
    "        with open('work_test/ntu/xview_bonevel_best_acc.pkl', 'wb') as f:\n",
    "            pickle.dump(score_dict, f)\n",
    "    return top5_acc, loss, top1_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78031e0e-f212-41a2-8fe9-d2eb91a637b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, bone, vel, batch_size=32):\n",
    "    print(f'data/ntu/NTU60_C{data}.npz', end=' ')\n",
    "    print(\"bone:\", bone, \"vel:\", vel) \n",
    "    data_loader = {}\n",
    "    # train\n",
    "    data_loader['train'] = torch.utils.data.DataLoader(\n",
    "        dataset=Feeder(f'data/ntu/NTU60_C{data}.npz', split='train', p_interval =[0.5, 1], window_size=64, bone=bone, vel=vel),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        worker_init_fn=init_seed)\n",
    "    # test\n",
    "    data_loader['test'] = torch.utils.data.DataLoader(\n",
    "        dataset=Feeder(f'data/ntu/NTU60_C{data}.npz', split='test', p_interval =[0.95], window_size=64, bone=bone, vel=vel),\n",
    "        batch_size=batch_size*2,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        worker_init_fn=init_seed)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7662e-6ac5-41a2-84ba-7c69c6d918f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def adjust_learning_rate(epoch):\n",
    "    global lr, warm_epochs, lr_decay_rate, step\n",
    "    if epoch <= warm_epochs:\n",
    "        lr = base_lr * epoch / warm_epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        if sched == 'plat':\n",
    "            scheduler.step(tst[1])\n",
    "        elif sched == 'cos' or sched == 'lin':\n",
    "            scheduler.step()\n",
    "        elif sched == 'custom':\n",
    "            lr = base_lr * (lr_decay_rate ** np.sum(epoch >= np.array(step)))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3d47e-d029-44d0-abdf-296261f90423",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_seed(2)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "phase = 'train'\n",
    "optimz = 'SGD'\n",
    "warm_epochs = 5\n",
    "epochs = 140\n",
    "batch_size = 64\n",
    "out_channels = 48\n",
    "dataset = 'S'\n",
    "bone, vel = False, False\n",
    "\n",
    "use_wandb = False\n",
    "\n",
    "# Optimizers\n",
    "if optimz == 'Adam' or optimz == 'AdamW' or optimz == 'NAdam':\n",
    "    base_lr = 0.001\n",
    "    lr = base_lr\n",
    "    weight_decay = 0.1\n",
    "    lr_decay_rate = 0.03\n",
    "    sched = 'cos'\n",
    "    lin_fin_epoch = 70\n",
    "elif optimz == 'SGD':\n",
    "    base_lr = 0.025\n",
    "    lr = base_lr\n",
    "    weight_decay = 0.0006\n",
    "    lr_decay_rate = 0.1\n",
    "    step = [110, 120]\n",
    "    sched = 'custom'\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Sanzhar_URIS\",\n",
    "    \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"Mamba\",\n",
    "        \"dataset\": \"NTU RGB+D 60\",\n",
    "        \"epochs\": epochs,\n",
    "        \"out_channels\": out_channels,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"lr_decay_rate\": lr_decay_rate,\n",
    "        \"optimizer\": optimz,\n",
    "        \"scheduler\": sched,\n",
    "        \"dataset\": dataset,\n",
    "        \"bone\": bone,\n",
    "        \"vel\": vel\n",
    "        }\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphModel(3, out_channels).to(device)\n",
    "# model.apply(init_weights)\n",
    "'''devices = [2, 3]\n",
    "device = devices[0]\n",
    "model = GraphModel(3, out_channels)\n",
    "model = nn.DataParallel(model,\n",
    "                        device_ids=devices,\n",
    "                        output_device=device)\n",
    "model.to(device)'''\n",
    "\n",
    "# Optimizer\n",
    "if optimz == 'SGD':\n",
    "    optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                momentum=0.9,\n",
    "                lr=base_lr,\n",
    "                nesterov = True,\n",
    "                weight_decay = weight_decay)\n",
    "elif optimz == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=base_lr)\n",
    "elif optimz == 'NAdam':\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "elif optimz == 'AdamW':\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Loss\n",
    "lossC = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Scheduler\n",
    "if sched == 'plat':\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_rate, patience=5)\n",
    "elif sched == 'cos':\n",
    "    scheduler = CosineAnnealingLR(optimizer, epochs, base_lr*lr_decay_rate)\n",
    "elif sched == 'lin':\n",
    "    scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=lr_decay_rate, total_iters=lin_fin_epoch)\n",
    "\n",
    "# loader\n",
    "data_loader = load_data(dataset, bone, vel, batch_size)\n",
    "\n",
    "if phase == 'train':\n",
    "    print(\"Parameters:\", count_parameters(model))\n",
    "    '''for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.numel())'''\n",
    "    print(model)\n",
    "    best_epoch_acc = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        adjust_learning_rate(epoch)\n",
    "        trn = train(epoch, loader=data_loader['train'])\n",
    "        if use_wandb:\n",
    "            tst = evalt(loader=data_loader['test'], model_name = f'Model{out_channels}_{wandb.run.name}')\n",
    "        else:\n",
    "            tst = evalt(loader=data_loader['test'], save = False)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train_loss\": trn[0], \"train_acc\": trn[1], \"test_acc\": tst[0], \"test_acc_1st\": tst[2], \"test_loss\": tst[1]})\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {trn[0]:.4f}, Train Acc: {trn[1]:.4f}, \\\n",
    "        Accuracy: {tst[2]:.4f}, Test_Loss: {tst[1]:.4f}, Current_Lr: {current_lr:.6f}')\n",
    "elif phase == 'test':\n",
    "    print(\"Parameters:\", count_parameters(model))\n",
    "    weights_path = 'mainruns/Model96_light-bee-139.pt'\n",
    "    model.load_state_dict(torch.load(weights_path, weights_only=True))\n",
    "    wf = weights_path.replace('.pt', '_wrong.txt')\n",
    "    rf = weights_path.replace('.pt', '_right.txt')\n",
    "    evalt(loader=data_loader['test'], wrong_file=wf, result_file=rf, save=False, save_pickle=False)\n",
    "elif phase == 'flops':\n",
    "    #from ptflops import get_model_complexity_in\n",
    "    print(\"Parameters:\", count_parameters(model))\n",
    "    weights_path = 'mainrjupuns/Model160_rich-terrain-231.pt'\n",
    "    model.load_state_dict(torch.load(weights_path, weights_only=True))\n",
    "    # n, c, t, v, m\n",
    "    #x = torch.randn([1, 3, 64, 25, 2]).to(device)  \n",
    "    Flops, params = get_model_complexity_info(model,  tuple([3, 64, 25, 2]), as_strings=False, print_per_layer_stat=False, verbose=False)\n",
    "    print(f\"Model stats: GFlops: {Flops*1e-9}, and (M) params: {params*1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc7eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798225ed-8ee2-44ec-a724-382d278481a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff6836-6547-47a5-a7bd-6ee872115bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''h2_forward = x.permute(0, 2, 3, 1).contiguous()\n",
    "        h2_backward = torch.flip(h2_forward, [2]).view(N, M * T * V, C)\n",
    "        h2 = torch.cat((h2_forward.view(N, M * T * V, C), h2_backward), dim=1)\n",
    "        with torch.cuda.device(x.device):\n",
    "            h2 = self.self_attn(h2)\n",
    "        h2_forward, h2_backward = h2.chunk(2, dim=1)\n",
    "        h2_backward = torch.flip(h2_backward.reshape(N * M, T, V, C), [2])\n",
    "        h2 = h2_forward.reshape(N * M, T, V, C) + h2_backward\n",
    "        h2 = h2.contiguous().permute(0, 3, 1, 2)'''\n",
    "'''\n",
    "        # linear transform\n",
    "        h1 = self.conv(x).permute(0, 2, 3, 1)\n",
    "\n",
    "        # k-hops\n",
    "        pos_emb = self.rpe[self.hops].permute(2, 0, 1)\n",
    "        pos_emb = torch.einsum('nctv, cvw -> nctw', x, pos_emb).permute(0, 2, 3, 1)\n",
    "    \n",
    "        # hyperedge   \n",
    "        label = F.one_hot(torch.tensor(self.joint_label)).float().to(x.device)\n",
    "        z = x @ (label / label.sum(dim=0, keepdim=True))\n",
    "        z = self.pe_proj(z).permute(3, 0, 1, 2)\n",
    "        # n=1, c=2, t=3, v=0\n",
    "        e = z[self.joint_label].permute(1, 3, 0, 2).contiguous()\n",
    "        \n",
    "        h_ov = torch.cat((pos_emb, h1, e), dim=1)\n",
    "        h_ov = h_ov.reshape(N, -1, C)\n",
    "        with torch.cuda.device(x.device):\n",
    "            h_ov = self.self_attn(h_ov)\n",
    "        h_ov1, h_ov2, h_ov3 = h_ov.reshape(N*M, 3*T, V, C).chunk(3, dim=1)\n",
    "        h_ov = (h_ov1 + h_ov2 + h_ov3).permute(0, 3, 1, 2)\n",
    "        out = self.norm3(h_ov + x).contiguous()\n",
    "        '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
