{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beebe9cf-d949-440c-8c8a-de91498d55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import traceback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from feeders.feeder_ntu import Feeder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "import inspect\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Dropout, Linear, Sequential\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.resolver import (\n",
    "    activation_resolver,\n",
    "    normalization_resolver,\n",
    ")\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "from torch_geometric.utils import degree, sort_edge_index\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630af9b4-9dc7-4ad2-9ca8-d1e57dd285c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(seed):\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # torch.backends.cudnn.enabled = True\n",
    "    # training speed is too slow if set to True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # on cuda 11 cudnn8, the default algorithm is very slow\n",
    "    # unlike on cuda 10, the default works well\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78031e0e-f212-41a2-8fe9-d2eb91a637b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self, phase='train'):\n",
    "    if phase=='train':\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=Feeder('data/ntu/NTU60_CS.npz', split='train', p_interval =[0.5, 1], window_size=64),\n",
    "            batch_size=32,\n",
    "            num_workers=0,\n",
    "            worker_init_fn=init_seed)\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=Feeder('data/ntu/NTU60_CS.npz', split='test', p_interval =[0.95], window_size=64),\n",
    "            batch_size=32,\n",
    "            num_workers=0,\n",
    "            worker_init_fn=init_seed)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94eea04d-f170-4b09-8fc2-dee66e57a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPSConv(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        conv: Optional[MessagePassing],\n",
    "        heads: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        act: str = 'relu',\n",
    "        att_type: str = 'transformer',\n",
    "        order_by_degree: bool = False,\n",
    "        shuffle_ind: int = 0,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        act_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        norm: Optional[str] = 'batch_norm',\n",
    "        norm_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.conv = conv\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.att_type = att_type\n",
    "        self.shuffle_ind = shuffle_ind\n",
    "        self.order_by_degree = order_by_degree\n",
    "        \n",
    "        assert (self.order_by_degree==True and self.shuffle_ind==0) or (self.order_by_degree==False), f'order_by_degree={self.order_by_degree} and shuffle_ind={self.shuffle_ind}'\n",
    "        \n",
    "        if self.att_type == 'transformer':\n",
    "            self.attn = torch.nn.MultiheadAttention(\n",
    "                channels,\n",
    "                heads,\n",
    "                dropout=attn_dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        if self.att_type == 'mamba':\n",
    "            self.self_attn = Mamba(\n",
    "                d_model=channels,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=1\n",
    "            )\n",
    "            \n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels * 2),\n",
    "            activation_resolver(act, **(act_kwargs or {})),\n",
    "            Dropout(dropout),\n",
    "            Linear(channels * 2, channels),\n",
    "            Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        norm_kwargs = norm_kwargs or {}\n",
    "        self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)\n",
    "\n",
    "        self.norm_with_batch = False\n",
    "        if self.norm1 is not None:\n",
    "            signature = inspect.signature(self.norm1.forward)\n",
    "            self.norm_with_batch = 'batch' in signature.parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        if self.conv is not None:\n",
    "            self.conv.reset_parameters()\n",
    "        self.attn._reset_parameters()\n",
    "        reset(self.mlp)\n",
    "        if self.norm1 is not None:\n",
    "            self.norm1.reset_parameters()\n",
    "        if self.norm2 is not None:\n",
    "            self.norm2.reset_parameters()\n",
    "        if self.norm3 is not None:\n",
    "            self.norm3.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Adj,\n",
    "        batch: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Runs the forward pass of the module.\"\"\"\n",
    "        hs = []\n",
    "        if self.conv is not None:  # Local MPNN.\n",
    "            h = self.conv(x, edge_index, **kwargs)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            h = h + x\n",
    "            if self.norm1 is not None:\n",
    "                if self.norm_with_batch:\n",
    "                    h = self.norm1(h, batch=batch)\n",
    "                else:\n",
    "                    h = self.norm1(h)\n",
    "            hs.append(h)\n",
    "\n",
    "        ### Global attention transformer-style model.\n",
    "        if self.att_type == 'transformer':\n",
    "            h, mask = to_dense_batch(x, batch)\n",
    "            h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)\n",
    "            h = h[mask]\n",
    "            \n",
    "        if self.att_type == 'mamba':\n",
    "            \n",
    "            if self.order_by_degree:\n",
    "                deg = degree(edge_index[0], x.shape[0]).to(torch.long)\n",
    "                order_tensor = torch.stack([batch, deg], 1).T\n",
    "                _, x = sort_edge_index(order_tensor, edge_attr=x)\n",
    "                \n",
    "            if self.shuffle_ind == 0:\n",
    "                h, mask = to_dense_batch(x, batch)\n",
    "                h = self.self_attn(h)[mask]\n",
    "            else:\n",
    "                mamba_arr = []\n",
    "                for _ in range(self.shuffle_ind):\n",
    "                    h_ind_perm = permute_within_batch(x, batch)\n",
    "                    h_i, mask = to_dense_batch(x[h_ind_perm], batch)\n",
    "                    h_i = self.self_attn(h_i)[mask][h_ind_perm]\n",
    "                    mamba_arr.append(h_i)\n",
    "                h = sum(mamba_arr) / self.shuffle_ind\n",
    "        ###\n",
    "        \n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = h + x  # Residual connection.\n",
    "        if self.norm2 is not None:\n",
    "            if self.norm_with_batch:\n",
    "                h = self.norm2(h, batch=batch)\n",
    "            else:\n",
    "                h = self.norm2(h)\n",
    "        hs.append(h)\n",
    "\n",
    "        out = sum(hs)  # Combine local and global outputs.\n",
    "\n",
    "        out = out + self.mlp(out)\n",
    "        if self.norm3 is not None:\n",
    "            if self.norm_with_batch:\n",
    "                out = self.norm3(out, batch=batch)\n",
    "            else:\n",
    "                out = self.norm3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.channels}, '\n",
    "                f'conv={self.conv}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf09edbe-6945-42d9-a42f-7c4ca199713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        # adjust padding for kernel size so that it will be equal to out_channe;s\n",
    "        pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            # kernel_size, 1 so that we look only for spatial\n",
    "            # 3 time steps windows of only 1 node\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1),\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1, 2, 3, 4],\n",
    "                 residual=False,\n",
    "                 residual_kernel_size=1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        # + 2 because we have additional 2 branches for max and 1x1 branch\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "        if type(kernel_size) == list:\n",
    "            assert len(kernel_size) == len(dilations)\n",
    "        else:\n",
    "            kernel_size = [kernel_size] * len(dilations)\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0\n",
    "                ),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=ks,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation\n",
    "                ),\n",
    "            )\n",
    "            # checking for each dilation so that we will look for global context\n",
    "            for ks, dilation in zip(kernel_size, dilations)\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(branch_channels)  # 为什么还要加bn\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride, 1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "        # print(len(self.branches))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,3,1,2)\n",
    "        # Input dim: (N,C,T,V)\n",
    "        res = self.residual(x)\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "\n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        out += res\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2790b42b-faad-477d-a39a-a1dc7122c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEnt(nn.Module):\n",
    "    def __init__(self, dim_in, dim, num_points=25):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_in = dim_in\n",
    "        nn1 = Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "            )\n",
    "        self.conv = GPSConv(dim_in, GINEConv(nn1), heads=4, attn_dropout=0.5,\n",
    "                               att_type='mamba',\n",
    "                               shuffle_ind=0,\n",
    "                               order_by_degree=True,\n",
    "                               d_state=16, d_conv=4)\n",
    "        self.lin = nn.Linear(dim_in, dim)\n",
    "        self.edge_emb = nn.Embedding(num_points*2, dim_in)\n",
    "        self_link = [(i, i) for i in range(25)]\n",
    "        inward_ori_index = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21), (6, 5), (7, 6),\n",
    "                    (8, 7), (9, 21), (10, 9), (11, 10), (12, 11), (13, 1),\n",
    "                    (14, 13), (15, 14), (16, 15), (17, 1), (18, 17), (19, 18),\n",
    "                    (20, 19), (22, 8), (23, 8), (24, 12), (25, 12)]\n",
    "        inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "        outward = [(j, i) for (i, j) in inward]\n",
    "        self.neighbor = inward + outward\n",
    "        global device\n",
    "        self.edge_index = self.convert_neighbor_to_edge_index(self.neighbor, device)\n",
    "\n",
    "    def forward(self, x, dims):\n",
    "        # N*M, T, V, C\n",
    "        N, C, T, V, M = dims\n",
    "        edge_attr = torch.ones(self.edge_index.size(1), dtype=torch.int, device=x.device)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "        batch = self.create_batch_array(N, M, T, V, x.device)\n",
    "        '''N * M - number of video sequences with person number\n",
    "        C - number of channels (3d position of points)\n",
    "        T - number of frames\n",
    "        V - number of skeleton points (25)\n",
    "        order is: N*M, T, V, C\n",
    "        '''\n",
    "        _, T, V, C = x.size()\n",
    "        x = x.view(-1, C)\n",
    "        x = self.conv(x, self.edge_index, batch, edge_attr=edge_attr)\n",
    "        x = x.view(N*M, T, V, C)\n",
    "        if self.dim != self.dim_in:\n",
    "            x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def convert_neighbor_to_edge_index(self, neighbor, device):\n",
    "        indices = torch.tensor(neighbor, dtype=torch.int64, device=device).t()\n",
    "        return indices\n",
    "\n",
    "    def create_batch_array(self, N, M, T, V, device):\n",
    "        # Total number of unique indices\n",
    "        num_indices = N * M \n",
    "        # Create a tensor of shape (num_indices, V) where each row contains the same index\n",
    "        batch = torch.arange(num_indices, dtype=torch.int64, device=device).repeat_interleave(V*T)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86eeaea-fc3b-48ea-97b6-fdf8a3b5e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim = dim\n",
    "        self.conv = GraphEnt(dim_in, dim)\n",
    "        if dim_in != dim:\n",
    "            self.lin = nn.Linear(dim_in, dim)\n",
    "        self.tcn = MultiScale_TemporalConv(dim, dim, kernel_size=5, stride=1,\n",
    "                                            dilations=[1,2],\n",
    "                                            # residual=True has worse performance in the end\n",
    "                                            residual=False)\n",
    "        self.act = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, dims):\n",
    "        if self.dim_in != self.dim:\n",
    "            x = self.act(self.tcn(self.conv(x, dims))+self.lin(x))\n",
    "        else:\n",
    "            x = self.act(self.tcn(self.conv(self.norm(x), dims)) + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653e9fba-b372-4a16-91b2-372abf348391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, dim_in, dim):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(9):\n",
    "            conv = GraphTCN(dim, dim)\n",
    "            self.convs.append(conv)\n",
    "        self.graph_tcn = GraphTCN(dim_in, dim)\n",
    "        self.fc1 = nn.Linear(dim, 60)\n",
    "        self.mlp = nn.Sequential(\n",
    "            self.fc1,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, T, V, M = x.size()\n",
    "        dims = x.size()\n",
    "        x = x.permute(0, 4, 2, 3, 1).contiguous().view(N, M, V, C, T).contiguous().view(N * M, T, V, C)\n",
    "        # N*M, T, V, C\n",
    "        x = self.graph_tcn(x, dims)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, dims)\n",
    "        '''\n",
    "        order is: N*M, T, V, C\n",
    "        '''\n",
    "        x = x.permute(0,3,1,2)\n",
    "        _, C, T, V = x.size()\n",
    "        x = x.view(N, M, C, -1)\n",
    "        # order is: N, M, C, T*V\n",
    "        x = x.mean(3).mean(1)\n",
    "        x = self.mlp(x)\n",
    "        #print(\"RESULT\", x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5af8e0-4184-45c4-b731-62f2b96289ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_value = []\n",
    "    acc_value = []\n",
    "    train_loader = load_data('train')\n",
    "    process = tqdm(train_loader, ncols=80)\n",
    "    for batch_idx, (data, label, index) in enumerate(process):\n",
    "        with torch.no_grad():\n",
    "            data = data.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(data)\n",
    "            loss = lossC(out, target=label)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loss_value.append(loss.data.item())\n",
    "        value, predict_label = torch.max(out.data, 1)\n",
    "        acc = torch.mean((predict_label == label.data).float())\n",
    "        acc_value.append(acc.data.item())\n",
    "    if epoch % 10 == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        weights = OrderedDict([[k.split('module.')[-1], v.cpu()] for k, v in state_dict.items()])\n",
    "        torch.save(weights, 'mainruns/Model-' + str(epoch) + '.pt')\n",
    "    return np.nanmean(loss_value), np.nanmean(acc_value)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d66187d-dc25-421a-b1a7-d4d553fe003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loader = load_data('test')\n",
    "    loss_value = []\n",
    "    score_frag = []\n",
    "    process = tqdm(test_loader, ncols=80)\n",
    "    for batch_idx, (data, label, index) in enumerate(process):\n",
    "        with torch.no_grad():\n",
    "            data = data.float().to(device)\n",
    "            label = label.long().to(device)\n",
    "            out = model(data)\n",
    "            loss = lossC(out, target=label)\n",
    "            #print(out.data.cpu().numpy())\n",
    "            value, predict_label = torch.max(out.data, 1)\n",
    "            score_frag.append(out.data.cpu().numpy())\n",
    "            loss_value.append(loss.data.item())\n",
    "    score = np.concatenate(score_frag)\n",
    "    loss = np.nanmean(loss_value)\n",
    "    best_acc = 0\n",
    "    for k in [1, 5]:\n",
    "        acc = test_loader.dataset.top_k(score, k) * 100\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "        print('\\tTop{}: {:.2f}%'.format(\n",
    "            k, acc))\n",
    "    return best_acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea3d47e-d029-44d0-abdf-296261f90423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 67535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspanchsan\u001b[0m (\u001b[33mspanchsan-hong-kong-polytechnic-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sabdrakhim/snap/snapd-desktop-integration/253/Desktop/up/URIS_HarmAssessment/wandb/run-20250121_015058-f7rhu2hd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/spanchsan-hong-kong-polytechnic-university/spanchsan-hong-kong-polytechnic-university/runs/f7rhu2hd' target=\"_blank\">lemon-tree-27</a></strong> to <a href='https://wandb.ai/spanchsan-hong-kong-polytechnic-university/spanchsan-hong-kong-polytechnic-university' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/spanchsan-hong-kong-polytechnic-university/spanchsan-hong-kong-polytechnic-university' target=\"_blank\">https://wandb.ai/spanchsan-hong-kong-polytechnic-university/spanchsan-hong-kong-polytechnic-university</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/spanchsan-hong-kong-polytechnic-university/spanchsan-hong-kong-polytechnic-university/runs/f7rhu2hd' target=\"_blank\">https://wandb.ai/spanchsan-hong-kong-polytechnic-university/spanchsan-hong-kong-polytechnic-university/runs/f7rhu2hd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.63it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 1.80%\n",
      "\tTop5: 11.14%\n",
      "Epoch: 01, Loss: 4.2597, Train Acc: 2.2471, Accuracy: 11.1372, Test_Loss: 4.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.72it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 2.44%\n",
      "\tTop5: 11.53%\n",
      "Epoch: 02, Loss: 4.0217, Train Acc: 4.7745, Accuracy: 11.5338, Test_Loss: 4.1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.64it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 4.05%\n",
      "\tTop5: 15.79%\n",
      "Epoch: 03, Loss: 3.9390, Train Acc: 5.9766, Accuracy: 15.7916, Test_Loss: 4.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.73it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 5.11%\n",
      "\tTop5: 20.44%\n",
      "Epoch: 04, Loss: 3.8600, Train Acc: 6.6579, Accuracy: 20.4435, Test_Loss: 3.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.61it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 5.54%\n",
      "\tTop5: 22.02%\n",
      "Epoch: 05, Loss: 3.7769, Train Acc: 7.6730, Accuracy: 22.0224, Test_Loss: 3.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.66it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 6.98%\n",
      "\tTop5: 25.00%\n",
      "Epoch: 06, Loss: 3.6948, Train Acc: 8.7060, Accuracy: 25.0031, Test_Loss: 3.7275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.72it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:02<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 7.49%\n",
      "\tTop5: 26.94%\n",
      "Epoch: 07, Loss: 3.6170, Train Acc: 10.0951, Accuracy: 26.9412, Test_Loss: 3.6589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.74it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 7.89%\n",
      "\tTop5: 28.32%\n",
      "Epoch: 08, Loss: 3.5397, Train Acc: 11.4349, Accuracy: 28.3231, Test_Loss: 3.6043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:21<00:00,  8.83it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 8.38%\n",
      "\tTop5: 30.39%\n",
      "Epoch: 09, Loss: 3.4597, Train Acc: 12.3531, Accuracy: 30.3859, Test_Loss: 3.5298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.70it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 9.24%\n",
      "\tTop5: 32.36%\n",
      "Epoch: 10, Loss: 3.3818, Train Acc: 13.2072, Accuracy: 32.3589, Test_Loss: 3.4733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.80it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 10.36%\n",
      "\tTop5: 35.06%\n",
      "Epoch: 11, Loss: 3.3070, Train Acc: 14.4068, Accuracy: 35.0577, Test_Loss: 3.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.76it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 11.17%\n",
      "\tTop5: 37.36%\n",
      "Epoch: 12, Loss: 3.2385, Train Acc: 15.5176, Accuracy: 37.3650, Test_Loss: 3.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:21<00:00,  8.86it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 11.95%\n",
      "\tTop5: 39.15%\n",
      "Epoch: 13, Loss: 3.1752, Train Acc: 16.5417, Accuracy: 39.1484, Test_Loss: 3.3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.72it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:01<00:00, 20.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 12.39%\n",
      "\tTop5: 40.56%\n",
      "Epoch: 14, Loss: 3.1181, Train Acc: 17.8037, Accuracy: 40.5627, Test_Loss: 3.2862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.76it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 13.58%\n",
      "\tTop5: 42.84%\n",
      "Epoch: 15, Loss: 3.0654, Train Acc: 18.8167, Accuracy: 42.8400, Test_Loss: 3.2399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.60it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 14.63%\n",
      "\tTop5: 44.36%\n",
      "Epoch: 16, Loss: 3.0167, Train Acc: 19.5998, Accuracy: 44.3591, Test_Loss: 3.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.73it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 15.40%\n",
      "\tTop5: 45.75%\n",
      "Epoch: 17, Loss: 2.9703, Train Acc: 20.5455, Accuracy: 45.7509, Test_Loss: 3.1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.78it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 15.81%\n",
      "\tTop5: 46.55%\n",
      "Epoch: 18, Loss: 2.9272, Train Acc: 21.4318, Accuracy: 46.5466, Test_Loss: 3.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.75it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 16.71%\n",
      "\tTop5: 47.93%\n",
      "Epoch: 19, Loss: 2.8881, Train Acc: 22.2589, Accuracy: 47.9285, Test_Loss: 3.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.62it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 16.96%\n",
      "\tTop5: 49.03%\n",
      "Epoch: 20, Loss: 2.8477, Train Acc: 23.1148, Accuracy: 49.0260, Test_Loss: 3.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:28<00:00,  8.44it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 17.56%\n",
      "\tTop5: 49.95%\n",
      "Epoch: 21, Loss: 2.8099, Train Acc: 23.8401, Accuracy: 49.9464, Test_Loss: 3.0403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.67it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:01<00:00, 20.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 18.15%\n",
      "\tTop5: 51.49%\n",
      "Epoch: 22, Loss: 2.7726, Train Acc: 24.8008, Accuracy: 51.4854, Test_Loss: 2.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.80it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 18.39%\n",
      "\tTop5: 52.40%\n",
      "Epoch: 23, Loss: 2.7370, Train Acc: 25.5340, Accuracy: 52.4033, Test_Loss: 2.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.78it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 18.89%\n",
      "\tTop5: 53.42%\n",
      "Epoch: 24, Loss: 2.7032, Train Acc: 26.2901, Accuracy: 53.4210, Test_Loss: 2.9398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.80it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 19.43%\n",
      "\tTop5: 54.87%\n",
      "Epoch: 25, Loss: 2.6677, Train Acc: 27.1655, Accuracy: 54.8652, Test_Loss: 2.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:26<00:00,  8.57it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 20.39%\n",
      "\tTop5: 56.39%\n",
      "Epoch: 26, Loss: 2.6344, Train Acc: 27.7317, Accuracy: 56.3867, Test_Loss: 2.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.79it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 20.94%\n",
      "\tTop5: 57.41%\n",
      "Epoch: 27, Loss: 2.6036, Train Acc: 28.5103, Accuracy: 57.4119, Test_Loss: 2.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.68it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 20.77%\n",
      "\tTop5: 57.58%\n",
      "Epoch: 28, Loss: 2.5736, Train Acc: 29.1687, Accuracy: 57.5765, Test_Loss: 2.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.68it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 21.53%\n",
      "\tTop5: 58.72%\n",
      "Epoch: 29, Loss: 2.5459, Train Acc: 29.8546, Accuracy: 58.7239, Test_Loss: 2.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.65it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 21.74%\n",
      "\tTop5: 58.76%\n",
      "Epoch: 30, Loss: 2.5165, Train Acc: 30.5134, Accuracy: 58.7563, Test_Loss: 2.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.68it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 22.51%\n",
      "\tTop5: 59.96%\n",
      "Epoch: 31, Loss: 2.4895, Train Acc: 31.2387, Accuracy: 59.9611, Test_Loss: 2.7420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.67it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 22.96%\n",
      "\tTop5: 60.87%\n",
      "Epoch: 32, Loss: 2.4637, Train Acc: 31.7824, Accuracy: 60.8690, Test_Loss: 2.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.70it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 23.68%\n",
      "\tTop5: 61.81%\n",
      "Epoch: 33, Loss: 2.4406, Train Acc: 32.2701, Accuracy: 61.8119, Test_Loss: 2.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:20<00:00,  8.91it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:02<00:00, 19.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 24.03%\n",
      "\tTop5: 62.12%\n",
      "Epoch: 34, Loss: 2.4140, Train Acc: 32.8438, Accuracy: 62.1187, Test_Loss: 2.6857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.62it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 24.41%\n",
      "\tTop5: 62.86%\n",
      "Epoch: 35, Loss: 2.3915, Train Acc: 33.4203, Accuracy: 62.8620, Test_Loss: 2.6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.80it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 25.20%\n",
      "\tTop5: 63.84%\n",
      "Epoch: 36, Loss: 2.3694, Train Acc: 33.9062, Accuracy: 63.8448, Test_Loss: 2.6409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.60it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 25.98%\n",
      "\tTop5: 64.65%\n",
      "Epoch: 37, Loss: 2.3484, Train Acc: 34.2633, Accuracy: 64.6479, Test_Loss: 2.6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:27<00:00,  8.51it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 26.61%\n",
      "\tTop5: 65.59%\n",
      "Epoch: 38, Loss: 2.3274, Train Acc: 34.8938, Accuracy: 65.5883, Test_Loss: 2.5855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.75it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 26.94%\n",
      "\tTop5: 65.75%\n",
      "Epoch: 39, Loss: 2.3064, Train Acc: 35.2036, Accuracy: 65.7479, Test_Loss: 2.5844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.65it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 27.37%\n",
      "\tTop5: 66.65%\n",
      "Epoch: 40, Loss: 2.2881, Train Acc: 35.7377, Accuracy: 66.6459, Test_Loss: 2.5591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.68it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 28.05%\n",
      "\tTop5: 67.13%\n",
      "Epoch: 41, Loss: 2.2684, Train Acc: 36.0570, Accuracy: 67.1348, Test_Loss: 2.5415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.69it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 28.37%\n",
      "\tTop5: 67.56%\n",
      "Epoch: 42, Loss: 2.2504, Train Acc: 36.5204, Accuracy: 67.5563, Test_Loss: 2.5284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.75it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 28.53%\n",
      "\tTop5: 67.96%\n",
      "Epoch: 43, Loss: 2.2336, Train Acc: 36.9269, Accuracy: 67.9579, Test_Loss: 2.5194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:22<00:00,  8.77it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 28.98%\n",
      "\tTop5: 68.61%\n",
      "Epoch: 44, Loss: 2.2169, Train Acc: 37.2772, Accuracy: 68.6064, Test_Loss: 2.4946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.69it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 29.07%\n",
      "\tTop5: 68.45%\n",
      "Epoch: 45, Loss: 2.2003, Train Acc: 37.6638, Accuracy: 68.4468, Test_Loss: 2.5075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.72it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 29.27%\n",
      "\tTop5: 68.76%\n",
      "Epoch: 46, Loss: 2.1885, Train Acc: 37.6817, Accuracy: 68.7611, Test_Loss: 2.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.73it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 29.75%\n",
      "\tTop5: 69.56%\n",
      "Epoch: 47, Loss: 2.1702, Train Acc: 38.1605, Accuracy: 69.5642, Test_Loss: 2.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:24<00:00,  8.66it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 29.47%\n",
      "\tTop5: 69.24%\n",
      "Epoch: 48, Loss: 2.1548, Train Acc: 38.3780, Accuracy: 69.2375, Test_Loss: 2.4839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:23<00:00,  8.71it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [01:00<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 30.05%\n",
      "\tTop5: 69.72%\n",
      "Epoch: 49, Loss: 2.1409, Train Acc: 38.6240, Accuracy: 69.7214, Test_Loss: 2.4619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1253/1253 [02:25<00:00,  8.63it/s]\n",
      "100%|███████████████████████████████████████| 1253/1253 [00:59<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1: 30.19%\n",
      "\tTop5: 69.95%\n",
      "Epoch: 50, Loss: 2.1275, Train Acc: 39.0076, Accuracy: 69.9509, Test_Loss: 2.4593\n"
     ]
    }
   ],
   "source": [
    "init_seed(2)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "out_channels = 20\n",
    "\n",
    "model = GraphModel(3, out_channels).to(device)\n",
    "'''devices = [3,2]\n",
    "device = devices[0]\n",
    "model = nn.DataParallel(GraphModel(3, 1024).to(device),\n",
    "                        device_ids=devices,\n",
    "                        output_device=device)'''\n",
    "optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=lr)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Parameters:\", count_parameters(model))\n",
    "lossC = nn.CrossEntropyLoss().to(device)\n",
    "use_amp = True\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"spanchsan-hong-kong-polytechnic-university\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"Mamba\",\n",
    "    \"dataset\": \"NTU RGB+D 60\",\n",
    "    \"epochs\": epochs,\n",
    "    \"out_channels\": out_channels,\n",
    "    \"batch_size\": 32\n",
    "    }\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    trn = train(epoch)\n",
    "    tst = test()\n",
    "    scheduler.step(tst[1])\n",
    "    wandb.log({\"train_loss\": trn[0], \"train_acc\": trn[1], \"test_acc\": tst[0], \"test_loss\": tst[1]})\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {trn[0]:.4f}, Train Acc: {trn[1]:.4f}, Accuracy: {tst[0]:.4f}, Test_Loss: {tst[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454ef46-f22b-43bb-aaf7-ec8c0b717877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
